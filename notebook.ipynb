{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc756a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loaded 62 journals from cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 62 journals\n",
      "  cell: Cell\n",
      "  cancer-cell: Cancer Cell\n",
      "  cell-chemical-biology: Cell Chemical Biology\n",
      "  cell-genomics: Cell Genomics\n",
      "  cell-host-microbe: Cell Host & Microbe\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(message)s'  # Simple format for cleaner output\n",
    ")\n",
    "\n",
    "# Import the async functions directly (NOT the regular crawler!)\n",
    "from src.papers_crawler.crawler_async import crawl_async, discover_journals_async\n",
    "\n",
    "# Discover available journals (use await since Colab runs in an async environment)\n",
    "journals = await discover_journals_async()\n",
    "print(f\"Found {len(journals)} journals\")\n",
    "\n",
    "# Show first 5 journals\n",
    "for slug, name in journals[:5]:\n",
    "    print(f\"  {slug}: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbeec282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Scanning 2 journal(s) for open access articles...\n",
      "\n",
      "ï¿½ Launching Firefox for journal: cell...\n",
      "\n",
      "ï¿½ Launching Firefox for journal: cell...\n",
      "âœ… Firefox browser ready for cell\n",
      "âœ… Firefox browser ready for cell\n",
      "ðŸ“‚ Journal folder: ./papers/cell\n",
      "ðŸ”Ž Crawling journal: cell at https://www.cell.com/cell/newarticles\n",
      "ðŸ“‚ Journal folder: ./papers/cell\n",
      "ðŸ”Ž Crawling journal: cell at https://www.cell.com/cell/newarticles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cookie consent button: button:has-text(\"Accept\")\n",
      "âœ“ Accepted cookie consent\n",
      "âœ“ Accepted cookie consent\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“š Found 38 open access articles in cell (will download up to 2)\n",
      "\n",
      "\u001b[A\n",
      "\u001b[AðŸ“„ Found open-access article: Anti-BCMA CAR-T therapy in patients with progressive multipl...\n",
      "ðŸ“„ Found open-access article: Anti-BCMA CAR-T therapy in patients with progressive multipl...\n",
      "\n",
      "\u001b[A\n",
      "\u001b[AðŸ“„ Found open-access article: The genomic footprints of wildSaccharumspecies trace domesti...\n",
      "ðŸ“„ Found open-access article: The genomic footprints of wildSaccharumspecies trace domesti...\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCancelledError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Crawl specific journals (use await)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m downloaded_files, articles = \u001b[38;5;28;01mawait\u001b[39;00m crawl_async(\n\u001b[32m      3\u001b[39m     year_from=\u001b[32m2020\u001b[39m,\n\u001b[32m      4\u001b[39m     year_to=\u001b[32m2025\u001b[39m,\n\u001b[32m      5\u001b[39m     out_folder=\u001b[33m\"\u001b[39m\u001b[33m./papers\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      6\u001b[39m     headless=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m      7\u001b[39m     limit=\u001b[32m2\u001b[39m,  \u001b[38;5;66;03m# limit per journal\u001b[39;00m\n\u001b[32m      8\u001b[39m     journal_slugs=[\u001b[33m\"\u001b[39m\u001b[33mcell\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mimmunity\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m      9\u001b[39m )\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDownloaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(downloaded_files)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m PDFs\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/papers_crawling_tool/src/papers_crawler/crawler_async.py:371\u001b[39m, in \u001b[36mcrawl_async\u001b[39m\u001b[34m(keywords, year_from, year_to, out_folder, headless, limit, journal_slugs, progress_callback, total_progress_callback)\u001b[39m\n\u001b[32m    367\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m cli_progress:\n\u001b[32m    368\u001b[39m     \u001b[38;5;66;03m# Update progress bar to show we're saving (force update)\u001b[39;00m\n\u001b[32m    369\u001b[39m     cli_progress.update(found_count, total_articles_found, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mðŸ’¾ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marticle_title[:\u001b[32m30\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33msaving\u001b[39m\u001b[33m\"\u001b[39m, force=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m371\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m download.save_as(dest_path)\n\u001b[32m    373\u001b[39m download_time = time.time() - download_start_time\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m os.path.exists(dest_path) \u001b[38;5;129;01mand\u001b[39;00m os.path.getsize(dest_path) > \u001b[32m1000\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/papers-crawling-tool-vEy1SkRO-py3.11/lib/python3.11/site-packages/playwright/async_api/_generated.py:7230\u001b[39m, in \u001b[36mDownload.save_as\u001b[39m\u001b[34m(self, path)\u001b[39m\n\u001b[32m   7212\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msave_as\u001b[39m(\u001b[38;5;28mself\u001b[39m, path: typing.Union[\u001b[38;5;28mstr\u001b[39m, pathlib.Path]) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   7213\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Download.save_as\u001b[39;00m\n\u001b[32m   7214\u001b[39m \n\u001b[32m   7215\u001b[39m \u001b[33;03m    Copy the download to a user-specified path. It is safe to call this method while the download is still in progress.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   7227\u001b[39m \u001b[33;03m        Path where the download should be copied.\u001b[39;00m\n\u001b[32m   7228\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m7230\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m mapping.from_maybe_impl(\u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._impl_obj.save_as(path=path))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/papers-crawling-tool-vEy1SkRO-py3.11/lib/python3.11/site-packages/playwright/_impl/_download.py:61\u001b[39m, in \u001b[36mDownload.save_as\u001b[39m\u001b[34m(self, path)\u001b[39m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msave_as\u001b[39m(\u001b[38;5;28mself\u001b[39m, path: Union[\u001b[38;5;28mstr\u001b[39m, Path]) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._artifact.save_as(path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/papers-crawling-tool-vEy1SkRO-py3.11/lib/python3.11/site-packages/playwright/_impl/_artifact.py:46\u001b[39m, in \u001b[36mArtifact.save_as\u001b[39m\u001b[34m(self, path)\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msave_as\u001b[39m(\u001b[38;5;28mself\u001b[39m, path: Union[\u001b[38;5;28mstr\u001b[39m, Path]) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     43\u001b[39m     stream = cast(\n\u001b[32m     44\u001b[39m         Stream,\n\u001b[32m     45\u001b[39m         from_channel(\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m             \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._channel.send(\n\u001b[32m     47\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33msaveAsStream\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     48\u001b[39m                 \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     49\u001b[39m             )\n\u001b[32m     50\u001b[39m         ),\n\u001b[32m     51\u001b[39m     )\n\u001b[32m     52\u001b[39m     make_dirs_for_file(path)\n\u001b[32m     53\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m stream.save_as(path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/papers-crawling-tool-vEy1SkRO-py3.11/lib/python3.11/site-packages/playwright/_impl/_connection.py:69\u001b[39m, in \u001b[36mChannel.send\u001b[39m\u001b[34m(self, method, timeout_calculator, params, is_internal, title)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msend\u001b[39m(\n\u001b[32m     62\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     63\u001b[39m     method: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     67\u001b[39m     title: \u001b[38;5;28mstr\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     68\u001b[39m ) -> Any:\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection.wrap_api_call(\n\u001b[32m     70\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mself\u001b[39m._inner_send(method, timeout_calculator, params, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[32m     71\u001b[39m         is_internal,\n\u001b[32m     72\u001b[39m         title,\n\u001b[32m     73\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/papers-crawling-tool-vEy1SkRO-py3.11/lib/python3.11/site-packages/playwright/_impl/_connection.py:556\u001b[39m, in \u001b[36mConnection.wrap_api_call\u001b[39m\u001b[34m(self, cb, is_internal, title)\u001b[39m\n\u001b[32m    554\u001b[39m \u001b[38;5;28mself\u001b[39m._api_zone.set(parsed_st)\n\u001b[32m    555\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m556\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m cb()\n\u001b[32m    557\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[32m    558\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m rewrite_error(error, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparsed_st[\u001b[33m'\u001b[39m\u001b[33mapiName\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/papers-crawling-tool-vEy1SkRO-py3.11/lib/python3.11/site-packages/playwright/_impl/_connection.py:123\u001b[39m, in \u001b[36mChannel._inner_send\u001b[39m\u001b[34m(self, method, timeout_calculator, params, return_as_dict)\u001b[39m\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[32m    120\u001b[39m callback = \u001b[38;5;28mself\u001b[39m._connection._send_message_to_server(\n\u001b[32m    121\u001b[39m     \u001b[38;5;28mself\u001b[39m._object, method, _augment_params(params, timeout_calculator)\n\u001b[32m    122\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m done, _ = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.wait(\n\u001b[32m    124\u001b[39m     {\n\u001b[32m    125\u001b[39m         \u001b[38;5;28mself\u001b[39m._connection._transport.on_error_future,\n\u001b[32m    126\u001b[39m         callback.future,\n\u001b[32m    127\u001b[39m     },\n\u001b[32m    128\u001b[39m     return_when=asyncio.FIRST_COMPLETED,\n\u001b[32m    129\u001b[39m )\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m callback.future.done():\n\u001b[32m    131\u001b[39m     callback.future.cancel()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/asyncio/tasks.py:428\u001b[39m, in \u001b[36mwait\u001b[39m\u001b[34m(fs, timeout, return_when)\u001b[39m\n\u001b[32m    425\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mPassing coroutines is forbidden, use tasks explicitly.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    427\u001b[39m loop = events.get_running_loop()\n\u001b[32m--> \u001b[39m\u001b[32m428\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m _wait(fs, timeout, return_when, loop)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/asyncio/tasks.py:535\u001b[39m, in \u001b[36m_wait\u001b[39m\u001b[34m(fs, timeout, return_when, loop)\u001b[39m\n\u001b[32m    532\u001b[39m     f.add_done_callback(_on_completion)\n\u001b[32m    534\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m535\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m waiter\n\u001b[32m    536\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    537\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout_handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mCancelledError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Crawl specific journals (use await)\n",
    "downloaded_files, articles = await crawl_async(\n",
    "    year_from=2020,\n",
    "    year_to=2025,\n",
    "    out_folder=\"./papers\",\n",
    "    headless=True,\n",
    "    limit=2,  # limit per journal\n",
    "    journal_slugs=[\"cell\", \"immunity\"],\n",
    ")\n",
    "\n",
    "print(f\"Downloaded {len(downloaded_files)} PDFs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4c5e07",
   "metadata": {},
   "source": [
    "## Testing with Updated Code\n",
    "\n",
    "The code has been updated to:\n",
    "- Remove unnecessary `IN_COLAB` checks and `await asyncio.sleep(0)` calls\n",
    "- Use `CLIProgressTracker` with tqdm disabled in Colab\n",
    "- Show detailed logger messages for debugging\n",
    "\n",
    "Restart the kernel and run the cells above to see the improved output."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "papers-crawling-tool-vEy1SkRO-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
